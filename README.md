# MAMGD_Optimizer
Gradient optimization method using exponential damping and second-order discrete derivative for neural networks and multidimensional real functions

* Sakovich, N.; Aksenov, D.; Pleshakova, E.; Gataullin, S. MAMGD: Gradient-Based Optimization Method Using Exponential Decay. Technologies 2024, 12, 154. https://doi.org/10.3390/technologies12090154

## Gradient-based Optimization Method for Multidimensional Real Functions and Neural Networks

This project focuses on the development of a gradient-based optimization method for multidimensional real functions and neural networks using exponential decay with Tensorflow and Keras.

## Technologies Used
- Tensorflow
- Keras
- Matplotlib
- NumPy
- Pandas
- Optuna

## Features
- Optimization of multidimensional real functions
- Optimization of neural networks
- Utilizes exponential decay for optimization
- Integration with Tensorflow and Keras for seamless implementation

## Algorithm

https://github.com/NekkittAY/MAMGD_Optimizer/blob/main/doc/MAMGD_optimizer_img.jpg?raw=true
